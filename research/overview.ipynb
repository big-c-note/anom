{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syscall Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a fun project, I decided to write a package for anomaly detection on my Linux server logs. A couple wants out of this:\n",
    "\n",
    "- Exploration of anomaly detection algo (written from scratch)\n",
    "- Use cpp and pybind11 to create the anomaly detection algo\n",
    "- Algo can be based off the one taught in Stanford Machine Learning class\n",
    "- Install tests with Travis or something like this\n",
    "- Use click for a command line interface\n",
    "- Use setup.py and release on PyPi\n",
    "- Use PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Always build one component from scratch. In this case, my anomaly detection model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syscall Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is there a way to do an online learning algo? Would need to regularly obsever the logs. How often is too often to tap into that feed?\n",
    "- I'll need to try to use PCA to reduce the dimentionality so that I can visualize this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm imagining starting with the techniques from my recent Stanford Machine Learning class. I can also look into libraries that already do this well for ease of implementation.\n",
    "- Otherwise this would include:\n",
    "    - lowercase\n",
    "    - strip symbols (some might be important)\n",
    "    - normalize features (see ip below)\n",
    "    - use a word stemmer\n",
    "    - get rid of white space and unnecessary characters\n",
    "- See ex6.pdf on my local drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace dates with a boolean for time of day potentially\n",
    "- Replace other highly specific data with a general indication of what it is\n",
    "    - One example may be to replace ips into two categories {mine, not_mine}\n",
    "    - Could consider trying Google embeddings on this.. not sure if that would do any good or not\n",
    "- Once I get all of my words, potentially discard the rarest ones, potentially not\n",
    "- If not, perhaps assign an integer to each frequently occuring word\n",
    "- Now, each log is given a 1 or 0 for each index in a vector whose features correspond to the vocabulary list we previously made, where\n",
    "\n",
    "$n$ = # of features, $x_i\\in{\\{0, 1\\}}$, each email is a vector $\\mathbb{R}^n$,\n",
    "$$x=\\left[\n",
    "\\begin{array}{ccc}\n",
    "   0 \\\\\n",
    "   1 \\\\\n",
    "   0 \\\\\n",
    "   \\vdots \\\\\n",
    "   0 \\\\\n",
    "   1 \\\\\n",
    "   1 \\\\\n",
    "\\end{array}\n",
    "\\right]=\\mathbb{R}^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Planning on mocking this up in python first for ease. Will do a simplified version of the Stanford ML class at first \n",
    "- One point of exploration will be whether or not the one I have in mind will work well for dummy data\n",
    "- See ex8.pdf and the associated code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an unlabled dataset which we expect to be largely not containing anomalies, $$\\{x^{(1)}, x^{(2)}, \\cdots, x^{(m)}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to estimate the parameters of the gaussian distribution $N(\\mu, \\sigma^2)$ where,\n",
    "\n",
    "$\\mu_j = \\frac{1}{m}\\sum_{i}^{m}x_i^j$ \n",
    "\n",
    "_the mean of feature j is eual to the sum over the training examples for that value divided by m_\n",
    "\n",
    "$\\sigma_j^2 = \\frac{1}{m}\\sum_{i}^{m}(x_i^j-\\mu_i^j)^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(x_i^j; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_i^j-\\mu_i^j)^2}{2\\sigma^2}}$\n",
    "\n",
    "$p(x)=\\Pi_{j=1}^n {p(x_j;\\mu_j,\\sigma^2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Anomaly Detection:\n",
    "Assumptions:\n",
    "- The data has a single gaussian cluster (not sure if that's how you say, but we would not expect the data to have several distinct groups)\n",
    "    - This may or may not be the case with this data. I can use PCA though to visualize in 2D and determine if I need to cluster first. If so, I would then simply find the examples that are furthest from the centers (potentially to which they belong).\n",
    "- The features are independent of each other.\n",
    "    - As I think about this, I do not at all think this is possible. certain words are going to be more likely to exist together\n",
    "- Each feature is normally distributed.\n",
    "    - Can plot a histogram to see, but this wouldn't be the case. If I plotted the expectation as a function of n randomly generated logs, I would have a gaussian, but I'm not sure that helps..\n",
    "    \n",
    "Multivariate Anomaly Detection\n",
    "- http://cs229.stanford.edu/section/gaussians.pdf\n",
    "\n",
    "After some reading of [non-white papers](https://stats.stackexchange.com/questions/62069/anomaly-detection-with-dummy-features-and-other-discrete-categorical-features), it might be ok to try the gaussian and just see how it goes! The PCA should allow me to visualize, but I think I should try without the PCA in the model at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Good is the System?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To decide this I'll actually need to create anomalous examples. This may prove difficult as I don't truly know what will constitute a security compromise\n",
    "- To start, I'll simply train on all data and see what happens. The logic here being, it would be still valuable to know which logs are unusual, whether it'll show security compromises, I have no idea!\n",
    "- To take it furhter, I can graph in 2D the data, and make sure I'm capturing what look like anomalies\n",
    "- To take it even furhter, I can train on 60% od data which is all non-anomalous, then 40% of data will be split equally into CV and test groups, where 50% of anomalous events are split between each. I'll train on Fscore on CV and see how it applies to the test group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finish math in this sheet for the basic one\n",
    "- Mock up a simple implementation with all prepackaged stuff (except for the anomaly detection model, which I want to do from scratch) and see how it goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anom",
   "language": "python",
   "name": "anom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
